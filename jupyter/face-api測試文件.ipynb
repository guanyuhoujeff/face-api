{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ef32e9-bba7-45b9-be6a-6ea8491894fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from faceapi import FaceAPIManager\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12712bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_api_manager = FaceAPIManager(\n",
    "    detector_model_path='model_weights/face-yolov8-m.pt',\n",
    "    encoder_model_path='model_weights/feifei_face.pt',\n",
    "    device='cuda'\n",
    ")\n",
    "# reid_encoder = FastReIDInterface(\n",
    "#     r'..\\package\\botsort\\fast_reid\\configs\\MOT17\\sbs_S50.yml', \n",
    "#     r'..\\package\\botsort\\reid_model\\mot17_sbs_S50.pth', \n",
    "#     face_api_manager._encoder._devie\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a97c862-1e9a-4708-b3ee-015dcc8c3f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "openvino_face_api_manager = FaceAPIManager(\n",
    "    detector_model_path='model_weights/face-yolov8-m_openvino_model/face-yolov8-m.xml',\n",
    "    encoder_model_path='model_weights/feifei_face_openvino_model/feifei_face.xml',\n",
    "    device='GPU'\n",
    ")\n",
    "# reid_encoder = FastReIDInterface(\n",
    "#     r'..\\package\\botsort\\fast_reid\\configs\\MOT17\\sbs_S50.yml', \n",
    "#     r'..\\package\\botsort\\reid_model\\mot17_sbs_S50.pth', \n",
    "#     face_api_manager._encoder._devie\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16172b6-1b9b-4676-b840-88747426a36e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe701a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_file = r'C:/Users/jeffg/face-project/Pytorch_Retinaface/data/FDDB/images/2002/12/10/big/img_410.jpg'\n",
    "# img_file = r'C:\\Users\\jeffg\\ai-data\\face-database\\classify - 複製\\9\\64fc0389b6db50bc1f756e0e-166-4154.png'\n",
    "img_file = r'C:\\Users\\jeffg\\ai-data\\face-database\\classify - 複製\\5\\64fbf58066959c5cc995539e-371-12697.png'\n",
    "# img_file = r'C:/Users/jeffg/Desktop/test.jpg'\n",
    "\n",
    "\n",
    "img = cv2.imread(img_file, cv2.IMREAD_COLOR)#[..., ::-1]\n",
    "display_img = img.copy()\n",
    "\n",
    "\n",
    "st = time.time()\n",
    "faces = face_api_manager.handle(img, conf=0.5)\n",
    "for face in faces:\n",
    "    pred_idx = face.embedding.argmax()\n",
    "    pred_prob = face.embedding[pred_idx]\n",
    "    print({\n",
    "       v: k\n",
    "       for k , v in face_api_manager._encoder.class_to_idx.items()\n",
    "    }[pred_idx], pred_prob)\n",
    "    \n",
    "    x1,y1,x2,y2  = face.xyxy  \n",
    "    #reid = reid_encoder.inference(img, np.array(face.xyxy)[None]).ravel()\n",
    "    cv2.rectangle(display_img, ( x1,y1), (x2,y2), (255, 255, 255))\n",
    "    break\n",
    "#embeddings = face_embedder.handle(face_img_list)\n",
    "plt.imshow(display_img[..., ::-1])\n",
    "\n",
    "cost = time.time() -st\n",
    "cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad96a83-bcb1-4b64-8f1d-0300971a8e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "face.xyxy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243e736c-5882-4adb-8d2a-a5f497d66860",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccff3b6e-c362-4086-aac2-05bdfb616ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_file = r'C:/Users/jeffg/face-project/Pytorch_Retinaface/data/FDDB/images/2002/12/10/big/img_410.jpg'\n",
    "# img_file = r'C:\\Users\\jeffg\\ai-data\\face-database\\classify - 複製\\9\\64fc0389b6db50bc1f756e0e-166-4154.png'\n",
    "img_file = r'C:\\Users\\jeffg\\ai-data\\face-database\\classify - 複製\\5\\64fbf58066959c5cc995539e-371-12697.png'\n",
    "# img_file = r'C:/Users/jeffg/Desktop/test.jpg'\n",
    "\n",
    "\n",
    "img = cv2.imread(img_file, cv2.IMREAD_COLOR)#[..., ::-1]\n",
    "# display_img = cv2.resize(img.copy(), (640,640))\n",
    "display_img = img.copy()\n",
    "\n",
    "\n",
    "st = time.time()\n",
    "faces = openvino_face_api_manager.handle(img, conf=0.5)\n",
    "for face in faces:\n",
    "    pred_idx = face.embedding.argmax()\n",
    "    pred_prob = face.embedding[pred_idx]\n",
    "    print({\n",
    "       v: k\n",
    "       for k , v in openvino_face_api_manager._encoder.class_to_idx.items()\n",
    "    }[pred_idx], pred_prob)\n",
    "    \n",
    "    x1,y1,x2,y2  = face.xyxy  \n",
    "    cv2.rectangle(display_img, ( x1,y1), (x2,y2), (255, 255, 255))\n",
    "    \n",
    "#embeddings = face_embedder.handle(face_img_list)\n",
    "plt.imshow(display_img[..., ::-1])\n",
    "\n",
    "cost = time.time() -st\n",
    "cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616cda81-2174-4ab6-b98d-a9521c50d61a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de6df80-dccf-43db-9768-0445aa78d83e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f18311-9a39-41a3-aa6d-462fd774d48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果是以embedding 來比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88de10de-1edd-410d-baa0-0f3efa7aaceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# from IPython import display\n",
    "# import numpy as np\n",
    "\n",
    "# img_dir = r'C:\\Users\\jeffg\\face-project\\facenet-pytorch\\data\\barai-data_cropped'\n",
    "# for label in os.listdir(img_dir):\n",
    "    \n",
    "#     train_label_similarity_list = []\n",
    "#     for train_img_path in os.listdir(os.path.join(img_dir, label)):\n",
    "#         img = cv2.imread(os.path.join(img_dir, label, train_img_path), cv2.IMREAD_COLOR)[..., ::-1]\n",
    "#         training_faces = face_api_manager.handle(img, conf=0.5)\n",
    "#         if len(training_faces) > 0:\n",
    "#             training_face = training_faces[0]\n",
    "#             similarity = cosine_similarity(face.embedding[None], training_face.embedding[None])\n",
    "#             train_label_similarity_list.append(similarity[0][0])\n",
    "            \n",
    "#     print('label : ', label) \n",
    "#     print(\"max s\", max(train_label_similarity_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f541d1c-3a93-4a48-8c42-df906a933f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e824de5-8526-4b0a-81cc-813e5a05be21",
   "metadata": {},
   "outputs": [],
   "source": [
    "while 1:\n",
    "    d, frame = cap.read()\n",
    "    if not d:\n",
    "        print('no image')\n",
    "        break\n",
    "\n",
    "    display_frame = frame.copy()\n",
    "    faces = openvino_face_api_manager.handle(frame, conf=0.5)\n",
    "    for face in faces:\n",
    "        \n",
    "        pred_idx = face.embedding.argmax()\n",
    "        # 如果爆掉，很大原因是因為不是finetune model\n",
    "        pred_prob = face.embedding[pred_idx]\n",
    "        label = {\n",
    "           v: k\n",
    "           for k , v in openvino_face_api_manager._encoder.class_to_idx.items()\n",
    "        }[pred_idx]\n",
    "        \n",
    "        #print(label, pred_prob)\n",
    "        x1,y1,x2,y2  = face.xyxy  \n",
    "        \n",
    "        cv2.putText(display_frame, f\"%s - %.2f\"%(label, pred_prob), (x1, y1),\n",
    "                    cv2.FONT_HERSHEY_DUPLEX, 0.5, (255, 255, 255))\n",
    "        cv2.rectangle(display_frame, ( x1,y1), (x2,y2), (255, 255, 255))\n",
    "    \n",
    "    cv2.imshow(\"camera\", display_frame)\n",
    "\n",
    "    \n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    \n",
    "    if key == ord('q'):  # q to quit\n",
    "        print('Quit')\n",
    "        break\n",
    "        \n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a15c4b-9a5e-4ad3-b39d-b030170744ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26508c7a-6428-43d9-baa5-bce580064ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "res[0]['det']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3aa79c-e295-41bb-92dd-beee3925afb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "669524e9-34c3-4a5c-8734-c8286e17df27",
   "metadata": {},
   "source": [
    "# Export model to openvino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50f264f-1cf3-423c-9cf2-f155b6af2341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install onnx==1.12.0\n",
    "# !pip install -q onnxruntime==1.17.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c800251f-4f67-427a-8919-b58aa622604b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 會存在 .\\model_weights 之下\n",
    "face_api_manager._detector._model.export(format='openvino', dynamic=True, half=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475903f6-941d-4294-b692-c5abe794aed0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9fa45ea9-52b3-402f-a880-579f293713d9",
   "metadata": {},
   "source": [
    "# 用 Openvino 讀取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2290d5-84fc-44b0-80e8-9f2748e530d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from ultralytics.utils import ops\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def letterbox(img: np.ndarray, new_shape:Tuple[int, int] = (640, 640), color:Tuple[int, int, int] = (114, 114, 114), auto:bool = False, scale_fill:bool = True, scaleup:bool = False, stride:int = 32):\n",
    "    \"\"\"\n",
    "    Resize image and padding for detection. Takes image as input, \n",
    "    resizes image to fit into new shape with saving original aspect ratio and pads it to meet stride-multiple constraints\n",
    "    \n",
    "    Parameters:\n",
    "      img (np.ndarray): image for preprocessing\n",
    "      new_shape (Tuple(int, int)): image size after preprocessing in format [height, width]\n",
    "      color (Tuple(int, int, int)): color for filling padded area\n",
    "      auto (bool): use dynamic input size, only padding for stride constrins applied\n",
    "      scale_fill (bool): scale image to fill new_shape\n",
    "      scaleup (bool): allow scale image if it is lower then desired input size, can affect model accuracy\n",
    "      stride (int): input padding stride\n",
    "    Returns:\n",
    "      img (np.ndarray): image after preprocessing\n",
    "      ratio (Tuple(float, float)): hight and width scaling ratio\n",
    "      padding_size (Tuple(int, int)): height and width padding size\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    # Resize and pad image while meeting stride-multiple constraints\n",
    "    shape = img.shape[:2]  # current shape [height, width]\n",
    "    if isinstance(new_shape, int):\n",
    "        new_shape = (new_shape, new_shape)\n",
    "\n",
    "    # Scale ratio (new / old)\n",
    "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
    "    if not scaleup:  # only scale down, do not scale up (for better test mAP)\n",
    "        r = min(r, 1.0)\n",
    "\n",
    "    # Compute padding\n",
    "    ratio = r, r  # width, height ratios\n",
    "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
    "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
    "    if auto:  # minimum rectangle\n",
    "        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n",
    "    elif scale_fill:  # stretch\n",
    "        dw, dh = 0.0, 0.0\n",
    "        new_unpad = (new_shape[1], new_shape[0])\n",
    "        ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n",
    "\n",
    "    dw /= 2  # divide padding into 2 sides\n",
    "    dh /= 2\n",
    "\n",
    "    if shape[::-1] != new_unpad:  # resize\n",
    "        img = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
    "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
    "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
    "    img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
    "    return img, ratio, (dw, dh)\n",
    "\n",
    "\n",
    "def preprocess_image(img0: np.ndarray):\n",
    "    \"\"\"\n",
    "    Preprocess image according to YOLOv8 input requirements. \n",
    "    Takes image in np.array format, resizes it to specific size using letterbox resize and changes data layout from HWC to CHW.\n",
    "    \n",
    "    Parameters:\n",
    "      img0 (np.ndarray): image for preprocessing\n",
    "    Returns:\n",
    "      img (np.ndarray): image after preprocessing\n",
    "    \"\"\"\n",
    "    # resize\n",
    "    img = letterbox(img0)[0]\n",
    "    \n",
    "    # Convert HWC to CHW\n",
    "    img = img.transpose(2, 0, 1)\n",
    "    img = np.ascontiguousarray(img)\n",
    "    return img\n",
    "\n",
    "\n",
    "def image_to_tensor(image:np.ndarray):\n",
    "    \"\"\"\n",
    "    Preprocess image according to YOLOv8 input requirements. \n",
    "    Takes image in np.array format, resizes it to specific size using letterbox resize and changes data layout from HWC to CHW.\n",
    "    \n",
    "    Parameters:\n",
    "      img (np.ndarray): image for preprocessing\n",
    "    Returns:\n",
    "      input_tensor (np.ndarray): input tensor in NCHW format with float32 values in [0, 1] range \n",
    "    \"\"\"\n",
    "    input_tensor = image.astype(np.float32)  # uint8 to fp32\n",
    "    input_tensor /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "    \n",
    "    # add batch dimension\n",
    "    if input_tensor.ndim == 3:\n",
    "        input_tensor = np.expand_dims(input_tensor, 0)\n",
    "    return input_tensor\n",
    "\n",
    "\n",
    "def postprocess(\n",
    "    pred_boxes:np.ndarray, \n",
    "    input_hw:Tuple[int, int], \n",
    "    orig_img:np.ndarray, \n",
    "    min_conf_threshold:float = 0.25, \n",
    "    nms_iou_threshold:float = 0.7, \n",
    "    agnosting_nms:bool = False, \n",
    "    max_detections:int = 300,\n",
    "    nc = 80\n",
    "):\n",
    "    \"\"\"\n",
    "    YOLOv8 model postprocessing function. Applied non maximum supression algorithm to detections and rescale boxes to original image size\n",
    "    Parameters:\n",
    "        pred_boxes (np.ndarray): model output prediction boxes\n",
    "        input_hw (np.ndarray): preprocessed image\n",
    "        orig_image (np.ndarray): image before preprocessing\n",
    "        min_conf_threshold (float, *optional*, 0.25): minimal accepted confidence for object filtering\n",
    "        nms_iou_threshold (float, *optional*, 0.45): minimal overlap score for removing objects duplicates in NMS\n",
    "        agnostic_nms (bool, *optiona*, False): apply class agnostinc NMS approach or not\n",
    "        max_detections (int, *optional*, 300):  maximum detections after NMS\n",
    "    Returns:\n",
    "       pred (List[Dict[str, np.ndarray]]): list of dictionary with det - detected boxes in format [x1, y1, x2, y2, score, label]\n",
    "    \"\"\"\n",
    "    nms_kwargs = {\"agnostic\": agnosting_nms, \"max_det\":max_detections}\n",
    "    preds = ops.non_max_suppression(\n",
    "        torch.from_numpy(pred_boxes),\n",
    "        min_conf_threshold,\n",
    "        nms_iou_threshold,\n",
    "        nc=nc,\n",
    "        **nms_kwargs\n",
    "    )\n",
    "\n",
    "    results = []\n",
    "    for i, pred in enumerate(preds):\n",
    "        shape = orig_img[i].shape if isinstance(orig_img, list) else orig_img.shape\n",
    "        if not len(pred):\n",
    "            results.append({\"det\": [], \"segment\": []})\n",
    "            continue\n",
    "        pred[:, :4] = ops.scale_boxes(input_hw, pred[:, :4], shape).round()\n",
    "        results.append({\"det\": pred})\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b53f5dc-7348-4aef-b567-631d0d8876e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae00b80-775b-47e0-ab0f-49e9d761cedd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fb9965-18aa-43c1-a981-ba3258655c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openvino as ov\n",
    "\n",
    "# Create OpenVINO Core object instance\n",
    "core = ov.Core()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e841dd4-e8af-40e3-b1c9-423791eb8d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "device = widgets.Dropdown(\n",
    "    options=core.available_devices + [\"AUTO\"],\n",
    "    value='AUTO',\n",
    "    description='Device:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcbf351-9226-4dbf-956e-24f10921eddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "det_model_path = 'model_weights/face-yolov8-m_openvino_model/face-yolov8-m.xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223f35d6-4f67-4e96-8a06-78d707bd9306",
   "metadata": {},
   "outputs": [],
   "source": [
    "det_ov_model = core.read_model(det_model_path)\n",
    "if device.value != \"CPU\":\n",
    "    det_ov_model.reshape({0: [1, 3, 640, 640]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9869cb24-31ad-4d01-b306-c89f06ba654d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ov_config = {}\n",
    "if \"GPU\" in device.value or (\"AUTO\" in device.value and \"GPU\" in core.available_devices):\n",
    "    ov_config = {\"GPU_DISABLE_WINOGRAD_CONVOLUTION\": \"YES\"}\n",
    "det_compiled_model = core.compile_model(det_ov_model, device.value, ov_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909ffde6-fa7b-4aa3-a646-2ad3485b3db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_file = r'C:/Users/jeffg/face-project/Pytorch_Retinaface/data/FDDB/images/2002/12/10/big/img_410.jpg'\n",
    "# img_file = r'C:\\Users\\jeffg\\ai-data\\face-database\\classify - 複製\\9\\64fc0389b6db50bc1f756e0e-166-4154.png'\n",
    "# img_file = r'data/feifei-face/gy/0541810.jpg'\n",
    "# img_file = r'C:/Users/jeffg/Desktop/test.jpg'\n",
    "img = cv2.imread(img_file)#\n",
    "display_img = img.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4280f632-927b-401f-b65c-6630eb4e4e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect(image:np.ndarray, model:ov.Model):\n",
    "    \"\"\"\n",
    "    OpenVINO YOLOv8 model inference function. Preprocess image, runs model inference and postprocess results using NMS.\n",
    "    Parameters:\n",
    "        image (np.ndarray): input image.\n",
    "        model (Model): OpenVINO compiled model.\n",
    "    Returns:\n",
    "        detections (np.ndarray): detected boxes in format [x1, y1, x2, y2, score, label]\n",
    "    \"\"\"\n",
    "    preprocessed_image = preprocess_image(image)\n",
    "    input_tensor = image_to_tensor(preprocessed_image)\n",
    "    result = model(input_tensor)\n",
    "    boxes = result[model.output(0)]\n",
    "    input_hw = input_tensor.shape[2:]\n",
    "    detections = postprocess(pred_boxes=boxes, input_hw=input_hw, orig_img=image)\n",
    "    return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2110d530-9ba9-475a-b2e5-5e5694d03d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d375f0f3-3733-499b-a73e-dd6c72c986d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_image = preprocess_image(image)\n",
    "input_tensor = image_to_tensor(preprocessed_image)\n",
    "result = det_compiled_model(input_tensor)\n",
    "boxes = result[det_compiled_model.output(0)]\n",
    "input_hw = input_tensor.shape[2:]\n",
    "detections = postprocess(pred_boxes=boxes, input_hw=input_hw, orig_img=image, nc=1)\n",
    "detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beaf6795-70c7-4a4b-aa3c-b78297175f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "preprocessed_image = preprocess_image(image)\n",
    "input_tensor = image_to_tensor(preprocessed_image)\n",
    "result = det_compiled_model(input_tensor)\n",
    "boxes = result[det_compiled_model.output(0)]\n",
    "input_hw = input_tensor.shape[2:]\n",
    "detections = postprocess(pred_boxes=boxes, input_hw=input_hw, orig_img=image, nc=1)\n",
    "detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee483ba-a3f6-40a3-80c7-add41b90d543",
   "metadata": {},
   "outputs": [],
   "source": [
    "detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3645753e-c165-4e86-bb25-2a789e79f5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "det = detections[0]['det'].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e6edf4-8d08-4da9-a62c-4f8784fa9e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_img = img.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77330587-9033-45db-b049-100b3ba957eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for face in det:\n",
    "    x1,y1,x2,y2  = face[:4].astype(int)\n",
    "    cv2.rectangle(dis_img, ( x1,y1), (x2,y2), (255, 255, 255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b7450d-490c-406e-a60a-dfd86227adb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(dis_img[..., ::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf93a43a-c467-4f70-9a8b-a0670b665dc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da29a15e-cfcc-45b2-a6c9-b2f604c78801",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6764413b-a86c-459c-b9a9-823c7e4a5322",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a043d55d-104f-41d2-80e9-443f849172b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_file = r'C:/Users/jeffg/face-project/Pytorch_Retinaface/data/FDDB/images/2002/12/10/big/img_410.jpg'\n",
    "# img_file = r'C:\\Users\\jeffg\\ai-data\\face-database\\classify - 複製\\9\\64fc0389b6db50bc1f756e0e-166-4154.png'\n",
    "img_file = r'C:\\Users\\jeffg\\ai-data\\face-database\\classify - 複製\\5\\64fbf58066959c5cc995539e-371-12697.png'\n",
    "# img_file = r'C:/Users/jeffg/Desktop/test.jpg'\n",
    "\n",
    "\n",
    "img = cv2.imread(img_file, cv2.IMREAD_COLOR)#[..., ::-1]\n",
    "display_img = cv2.resize(img.copy(), (640,640))\n",
    "\n",
    "\n",
    "st = time.time()\n",
    "faces = openvino_face_api_manager.handle(img, conf=0.5)\n",
    "for face in faces:\n",
    "    pred_idx = face.embedding.argmax()\n",
    "    pred_prob = face.embedding[pred_idx]\n",
    "    print({\n",
    "       v: k\n",
    "       for k , v in openvino_face_api_manager._encoder.class_to_idx.items()\n",
    "    }[pred_idx], pred_prob)\n",
    "    \n",
    "    x1,y1,x2,y2  = face.xyxy  \n",
    "\n",
    "    o_w, o_h= img.shape[:2]\n",
    "\n",
    "    w_ratio = 640/o_w\n",
    "    h_ratio = 640/o_h\n",
    "\n",
    "    x1 = int(x1 * h_ratio)\n",
    "    x2 = int(x2 * h_ratio)\n",
    "    \n",
    "    #reid = reid_encoder.inference(img, np.array(face.xyxy)[None]).ravel()\n",
    "    cv2.rectangle(display_img, ( x1,y1), (x2,y2), (255, 255, 255))\n",
    "    break\n",
    "#embeddings = face_embedder.handle(face_img_list)\n",
    "plt.imshow(display_img[..., ::-1])\n",
    "\n",
    "cost = time.time() -st\n",
    "cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b6d9d8-2858-45e8-b8f1-a7b0fed544d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40d7c4a-67ea-4ce0-b79f-d7930ec1af34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27ba9d0-7623-4d59-a689-245a2276f875",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf458472-d472-4414-818a-d65d4243e3aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d50f289-9a4b-49a4-8b72-555fe1658988",
   "metadata": {},
   "source": [
    "# embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5553845-1015-4043-9120-059ef49591d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_api_manager._encoder._model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bdfc7e-8ec1-4d04-a6fc-19eda973caa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model_file_name = os.path.basename(face_api_manager._encoder._model_path)\n",
    "model_weight_path = os.path.dirname(face_api_manager._encoder._model_path)\n",
    "\n",
    "embedding_onnx_model_file_name = embedding_model_file_name.replace('.pt', '.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbbf088-e57b-48b7-b4d6-a2aa405f0ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_path = os.path.join(model_weight_path, embedding_onnx_model_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399e8535-00bc-4c78-8eff-7ff6f20a09e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = face_api_manager._encoder.preprocess([ cv2.imread(img_file, cv2.IMREAD_COLOR)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4b67d1-3523-4da7-baa4-0b03ebc0bb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(\n",
    "    face_api_manager._encoder._model,\n",
    "    dummy_input,\n",
    "    onnx_path,\n",
    "    export_params=True, \n",
    "      do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "      input_names = ['input'],   # the model's input names\n",
    "      output_names = ['output'], # the model's output names\n",
    "      dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes\n",
    "                    'output' : {0 : 'batch_size'}}\n",
    ")\n",
    "print(f\"ONNX model exported to {onnx_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c1a0cc-dddf-4bc5-9ecf-a1a99f02c7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ov_model = ov.convert_model(onnx_path)\n",
    "# ov.save_model(ov_model, ir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abddaa4-7843-44af-a369-345d5c3923e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "openvino_model_dir_path = onnx_path.replace('.onnx', '_openvino_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9340d79d-25ff-4fed-bf3b-ea589f873a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(openvino_model_dir_path):\n",
    "    os.makedirs(openvino_model_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c6c87d-6988-40d6-ad35-12065bce59b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "openvino_model_dir_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a95478-f60c-4cf3-b4f3-7c0e50f6d09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ir_path = os.path.join(openvino_model_dir_path, embedding_onnx_model_file_name.replace('.onnx', '.xml'))\n",
    "meta_path = os.path.join(openvino_model_dir_path, embedding_onnx_model_file_name.replace('.onnx', '.meta'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff50f01-4342-4e72-a842-b72d3eb45d2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b9a5a9-a71d-4757-86e8-44090febdf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "ov.save_model(ov_model, ir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d7c22c-3102-4e9f-87d8-11773337d2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ir_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c9d01a-2f35-47ac-9cc5-f1d4bb0473c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_onnx_model_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fc8f79-9331-4e4e-b8f3-2cb02f0e176c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c6cd70-39a0-4e7a-bd1d-0e08e24e21d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(meta_path, 'w') as writer:\n",
    "    writer.write(json.dumps(face_api_manager._encoder._model.class_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5e20fe-b232-49be-9d11-b2f25ed42254",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accb07d3-123e-45b6-8ec2-a328bcc7f923",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1847cf2e-a37b-4f5b-a874-96798aa5a2d8",
   "metadata": {},
   "source": [
    "# 測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5164390e-f953-4d7a-9d54-c56cafd72d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "device = widgets.Dropdown(\n",
    "    options=core.available_devices + [\"AUTO\"],\n",
    "    value='AUTO',\n",
    "    description='Device:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347c07ad-ef3f-4cef-bacc-c7c58f0829c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the network in OpenVINO Runtime.\n",
    "core = ov.Core()\n",
    "model_ir = core.read_model(model='model_weights/feifei_face_openvino_model/feifei_face.xml')\n",
    "\n",
    "\n",
    "meta_path = 'model_weights/feifei_face_openvino_model/feifei_face.meta'\n",
    "with open(meta_path, 'r') as reader:\n",
    "    meta_data = json.loads(reader.read())\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "compiled_model_ir = core.compile_model(model=model_ir, device_name=device.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78598df5-9b6d-4ce9-a99c-64a9ef557e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get input and output layers.\n",
    "output_layer_ir = compiled_model_ir.output(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0489d69-df9d-42c6-889c-ab2a30b8cf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d38e8a-b667-4ac3-ad0f-d1a322abf04d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345945e2-26d3-47f2-b6c5-cb139f5e4496",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_file = r'C:/Users/jeffg/face-project/Pytorch_Retinaface/data/FDDB/images/2002/12/10/big/img_410.jpg'\n",
    "# img_file = r'C:\\Users\\jeffg\\ai-data\\face-database\\classify - 複製\\9\\64fc0389b6db50bc1f756e0e-166-4154.png'\n",
    "# img_file = r'data/feifei-face/gy/0541810.jpg'\n",
    "# img_file = r'C:/Users/jeffg/Desktop/test.jpg'\n",
    "img = cv2.imread(img_file, cv2.IMREAD_COLOR)#\n",
    "display_img = img.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e557f9b6-bdae-41c1-8452-f0f539668a7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "st = time.time()\n",
    "faces = face_api_manager.handle(img, conf=0.5)\n",
    "for face in faces:\n",
    "    pred_idx = face.embedding.argmax()\n",
    "    pred_prob = face.embedding[pred_idx]\n",
    "    print({\n",
    "       v: k\n",
    "       for k , v in face_api_manager._encoder._model.class_to_idx.items()\n",
    "    }[pred_idx], pred_prob)\n",
    "    \n",
    "    x1,y1,x2,y2  = face.xyxy  \n",
    "    x1,y1,x2,y2  = face.xyxy\n",
    "    face_img = img[y1:y2,x1:x2,:]\n",
    "    cv2.rectangle(display_img, ( x1,y1), (x2,y2), (255, 255, 255))\n",
    "#embeddings = face_embedder.handle(face_img_list)\n",
    "plt.imshow(display_img[..., ::-1])\n",
    "\n",
    "cost = time.time() -st\n",
    "cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd824968-ff81-4daf-8345-17b532b4bc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a34eacf-02f4-4090-8b9a-40ec63ff5f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "face_img_list = [face_img]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a46240b-be08-4207-82a7-ef681b0125d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_input_image = torch.stack([face_api_manager._encoder.transform(im) for im in face_img_list]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5230b8a-8c31-4d76-bb08-8e528179bb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalized_input_image = face_api_manager._encoder.transform(face_img)[None].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9678a43e-a31e-45ed-9711-c15d8f9ebd98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88829c4-882b-425b-9b05-af0a143432a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_ir = compiled_model_ir(normalized_input_image)[output_layer_ir]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548c00e8-6067-4358-b472-a68ddf81d08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_ir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d64c7f-e6fd-420e-aaa4-ab0fdf5c2bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_idx = res_ir.argmax()\n",
    "pred_prob = res_ir[pred_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9db9c6-a027-4b30-aad0-df49ecf45041",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "       v: k\n",
    "       for k , v in face_api_manager._encoder._model.class_to_idx.items()\n",
    "    }[pred_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1021b4-0326-4e9f-b85c-8e613669b1c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596511b9-c387-4cfe-806f-01f81579693c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd680866-4a7b-4379-a447-9caa965d1f47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
