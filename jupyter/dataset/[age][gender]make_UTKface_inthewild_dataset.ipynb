{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vd59MkxsdgP0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jeffg\\AppData\\Local\\Temp\\ipykernel_47624\\1379983671.py:19: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # device object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用 UTKface_inthewild"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Labels\n",
    "The labels of each face image is embedded in the file name, formated like [age]_[gender]_[race]_[date&time].jpg\n",
    "\n",
    "[age] is an integer from 0 to 116, indicating the age\n",
    "[gender] is either 0 (male) or 1 (female)\n",
    "[race] is an integer from 0 to 4, denoting White, Black, Asian, Indian, and Others (like Hispanic, Latino, Middle Eastern).\n",
    "[date&time] is in the format of yyyymmddHHMMSSFFF, showing the date and time an image was collected to UTKFace\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_face_img_list = pd.read_pickle( r\"C:\\Users\\jeffg\\face-project\\face-api\\jupyter\\face_img_list.pkl\")\n",
    "raw_face_label_list = pd.read_pickle(r\"C:\\Users\\jeffg\\face-project\\face-api\\jupyter\\face_label_list.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61_1_20170109142408075.jpg\n",
      "61_3_20170109150557335.jpg\n",
      "39_1_20170116174525125.jpg\n"
     ]
    }
   ],
   "source": [
    "face_img_list = []\n",
    "face_label_list = []\n",
    "for idx, f_name in enumerate(raw_face_label_list):\n",
    "    if len(f_name.split(\"_\")) != 4:\n",
    "        print(f_name)\n",
    "    else:\n",
    "        face_label_list.append(raw_face_label_list[idx])\n",
    "        face_img_list.append(raw_face_img_list[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(face_img_list, face_label_list, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16108it [00:50, 319.04it/s]\n",
      "7934it [00:24, 322.89it/s]\n"
     ]
    }
   ],
   "source": [
    "for img_array, f_name in tqdm(zip(X_train, y_train)):\n",
    "    age_, gender_ = f_name.replace(\"__\", \"_\").split(\"_\")[:2]\n",
    "    \n",
    "    age = \"%d-%d\"%((int(age_)//5)*5 , (int(age_)//5 + 1)*5)\n",
    "    gender = [\"male\", \"female\"][int(gender_)]\n",
    "\n",
    "    img = Image.fromarray(img_array)\n",
    "\n",
    "    ## age\n",
    "    os.makedirs(os.path.join(\"UTKface_age\", \"train\", age) , exist_ok=True)\n",
    "    img.save(os.path.join(\"UTKface_age\", \"train\", age , f_name))\n",
    "\n",
    "    ## gender\n",
    "    os.makedirs(os.path.join(\"UTKface_gender\", \"train\", gender) , exist_ok=True)\n",
    "    img.save(os.path.join(\"UTKface_gender\", \"train\", gender , f_name))\n",
    "    \n",
    "\n",
    "for img_array, f_name in tqdm(zip(X_test, y_test)):\n",
    "    age_, gender_ = f_name.replace(\"__\", \"_\").split(\"_\")[:2]\n",
    "    \n",
    "    age = \"%d-%d\"%((int(age_)//5)*5 , (int(age_)//5 + 1)*5)\n",
    "    gender = [\"male\", \"female\"][int(gender_)]\n",
    "\n",
    "    img = Image.fromarray(img_array)\n",
    "\n",
    "    ## age\n",
    "    os.makedirs(os.path.join(\"UTKface_age\", \"test\", age) , exist_ok=True)\n",
    "    img.save(os.path.join(\"UTKface_age\", \"test\", age , f_name))\n",
    "\n",
    "    ## gender\n",
    "    os.makedirs(os.path.join(\"UTKface_gender\", \"test\", gender) , exist_ok=True)\n",
    "    img.save(os.path.join(\"UTKface_gender\", \"test\", gender , f_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "model = YOLO(r\"C:\\Users\\jeffg\\face-project\\age_gender\\Face-Gender-Classification-PyTorch\\runs\\classify\\UTKface_Gender\\weights\\best.pt\")  # load a pretrained model (recommended for training)\n",
    "\n",
    "# Train the model\n",
    "# results = model.train(data=\"UTKface_Age\", epochs=100, imgsz=640, name=\"UTKface_Age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now_path = os.getcwd()\n",
    "os.chdir(r\"..\\..\\face-api\\jupyter\")\n",
    "import __init__\n",
    "from faceapi import FaceAPIManager\n",
    "face_api_manager = FaceAPIManager(\n",
    "    detector_model_path=r'C:\\Users\\jeffg\\face-project\\face-api\\model_weights\\face-yolov8-m.pt',\n",
    "    encoder_model_path=r'C:\\Users\\jeffg\\face-project\\face-api\\model_weights\\feifei_face.pt',\n",
    "    emotion_model_path=r\"C:\\Users\\jeffg\\face-project\\face-api\\model_weights\\face-emotion.pth\",\n",
    "    \n",
    "    age_proto_path = r'C:\\Users\\jeffg\\face-project\\face-api\\model_weights\\age_gender_models\\age_detector\\age_deploy.prototxt', \n",
    "    age_model_path = r'C:\\Users\\jeffg\\face-project\\face-api\\model_weights\\age_gender_models\\age_detector\\age_net.caffemodel', \n",
    "    gender_proto_path = r'C:\\Users\\jeffg\\face-project\\face-api\\model_weights\\age_gender_models\\gender_detector\\gender_deploy.prototxt', \n",
    "    gender_model_path = r'C:\\Users\\jeffg\\face-project\\face-api\\model_weights\\age_gender_models\\gender_detector\\gender_net.caffemodel', \n",
    "\n",
    "    device='cuda'\n",
    ")\n",
    "os.chdir(now_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.isOpened()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "while True:\n",
    "    d, frame = cap.read()\n",
    "    display_frame = frame.copy()\n",
    "    faces = face_api_manager.handle(frame)\n",
    "    \n",
    "    label = \"None\"\n",
    "    if len(faces) > 0:\n",
    "        face = faces[0]\n",
    "\n",
    "        x1,y1,x2,y2  = face.xyxy\n",
    "        face_img = frame[y1:y2,x1:x2,:]\n",
    "\n",
    "        \n",
    "        res = model.predict(face_img, verbose=False)\n",
    "        probs = res[0].probs\n",
    "        prob = probs.top1conf.cpu().numpy().item()\n",
    "        label = \"male\" if prob >= 0.5 else \"female\" \n",
    "        label = f\"{label}-{prob}\"\n",
    "\n",
    "        #label = f\"%s-%s-%s, %s - %.2f\"%(face.emotion,  face.gender, face.age, label, pred_prob)\n",
    "    \n",
    "        cv2.putText(display_frame, label, (x1, y1),\n",
    "                    cv2.FONT_HERSHEY_DUPLEX, 0.5, (255, 255, 255))\n",
    "        cv2.rectangle(display_frame, ( x1,y1), (x2,y2), (255, 255, 255))\n",
    "        \n",
    "\n",
    "    cv2.imshow('Webcam', display_frame)\n",
    "\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.predict(face_img, verbose=False)\n",
    "probs = res[0].probs\n",
    "prob = probs.top1conf.cpu().numpy().item()\n",
    "label = \"male\" if prob >= 0.5 else \"female\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMKG5We5qk8WkqjTIIABfAo",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Face Gender Classification using Transfer Learning with ResNet18 (Acc. 97%)",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "147e673ba1c142e0b5e3a6c1dbd91db8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "392b87160a794522b21421b3b2cf1f13": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3dfe281dea5e44fd915ef88ebf8cf8a7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3f8eb54035ae4c2bb189089ccf18c4ab",
      "max": 46827520,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e634e445f0c14378b110adfb0475dba7",
      "value": 46827520
     }
    },
    "3f8eb54035ae4c2bb189089ccf18c4ab": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "610d3c208aff490c855d1c2a9c833416": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3dfe281dea5e44fd915ef88ebf8cf8a7",
       "IPY_MODEL_95eb44ab771e40dab8eba89a46f13bcb"
      ],
      "layout": "IPY_MODEL_a3d5539afe6a4869a53a1a192753b1e5"
     }
    },
    "95eb44ab771e40dab8eba89a46f13bcb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_147e673ba1c142e0b5e3a6c1dbd91db8",
      "placeholder": "​",
      "style": "IPY_MODEL_392b87160a794522b21421b3b2cf1f13",
      "value": " 44.7M/44.7M [09:38&lt;00:00, 81.0kB/s]"
     }
    },
    "a3d5539afe6a4869a53a1a192753b1e5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e634e445f0c14378b110adfb0475dba7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
