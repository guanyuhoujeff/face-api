{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66990d75-fd20-40ad-90ff-aa0a796264f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import __init__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef576b6c-1819-4f23-b9f0-186caad8ede8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from faceapi import FaceAPIManager\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52e20c4c-2336-499f-a70c-f8ea23b67196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: ÁÑ°Ê≥ïÂ≠òÂèñ '....model_weights': Ê≤íÊúâÊ≠§‰∏ÄÊ™îÊ°àÊàñÁõÆÈåÑ\n"
     ]
    }
   ],
   "source": [
    "ls ..\\..\\model_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9449b0ce-0a84-46d1-ad74-19ab232f3b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_api_manager = FaceAPIManager(\n",
    "    detector_model_path= r'..\\..\\model_weights\\face-yolov8-m.pt',\n",
    "    # encoder_model_path=r'C:\\Users\\jeffg\\face-project\\face-api\\model_weights\\feifei_face.pt',\n",
    "    emotion_model_path= r\"..\\..\\model_weights\\best-face_emotion_ResNet18.pth\", \n",
    "    age_model_path =  r\"..\\..\\model_weights\\best-face_age_ResNet18.pth\", \n",
    "    gender_model_path =  r\"..\\..\\model_weights\\best-face_gender_ResNet18.pth\", \n",
    "    device='cuda'\n",
    ")\n",
    "\n",
    "# reid_encoder = FastReIDInterface(\n",
    "#     r'..\\package\\botsort\\fast_reid\\configs\\MOT17\\sbs_S50.yml', \n",
    "#     r'..\\package\\botsort\\reid_model\\mot17_sbs_S50.pth', \n",
    "#     face_api_manager._encoder._devie\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b1f752-95f7-4989-bbfb-8068b7b5a122",
   "metadata": {},
   "source": [
    "# Export model to openvino"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609fa679-cd55-488b-828f-4069afa4b950",
   "metadata": {},
   "source": [
    "detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40ec3cb8-8b1d-448b-a27b-439df05a6f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.18 üöÄ Python-3.10.13 torch-2.2.1+cu121 CPU (12th Gen Intel Core(TM) i9-12900H)\n",
      "Model summary (fused): 218 layers, 25840339 parameters, 0 gradients, 78.7 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from '..\\..\\model_weights\\face-yolov8-m.pt' with input shape (1, 3, 1088, 1088) BCHW and output shape(s) (1, 5, 24276) (49.6 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.12.0 opset 17...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success ‚úÖ 1.7s, saved as '..\\..\\model_weights\\face-yolov8-m.onnx' (98.7 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mOpenVINO:\u001b[0m starting export with openvino 2023.3.0-13775-ceeafaf64f3-releases/2023/3...\n",
      "\u001b[34m\u001b[1mOpenVINO:\u001b[0m export success ‚úÖ 2.9s, saved as '..\\..\\model_weights\\face-yolov8-m_openvino_model\\' (98.9 MB)\n",
      "\n",
      "Export complete (7.8s)\n",
      "Results saved to \u001b[1mC:\\Users\\jeffg\\face-project\\face-api\\model_weights\u001b[0m\n",
      "Predict:         yolo predict task=detect model=..\\..\\model_weights\\face-yolov8-m_openvino_model imgsz=1088  \n",
      "Validate:        yolo val task=detect model=..\\..\\model_weights\\face-yolov8-m_openvino_model imgsz=1088 data=./yolo-wide-face/data.yaml  \n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'..\\\\..\\\\model_weights\\\\face-yolov8-m_openvino_model'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ÊúÉÂ≠òÂú® .\\model_weights ‰πã‰∏ã\n",
    "face_api_manager._detector._model.export(format='openvino', dynamic=True, half=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54df458e-a6bf-401b-a93d-f5347db51e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openvino as ov\n",
    "\n",
    "# Create OpenVINO Core object instance\n",
    "core = ov.Core()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49c15b1-86bf-436d-928b-354b7e1013af",
   "metadata": {},
   "source": [
    "emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79c7160e-bf73-41ec-acd0-bd56e6db3953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert model to openvino.runtime.Model object\n",
    "\n",
    "openvino_model_output_path = os.path.join(\n",
    "    os.path.dirname(face_api_manager._emotion_dector._torch_model_path),\n",
    "    \"face_emotion_openvino_model\"\n",
    ")\n",
    "\n",
    "\n",
    "# Save openvino.runtime.Model object on disk\n",
    "emotion_ov_model = ov.convert_model(face_api_manager._emotion_dector._model)\n",
    "ov.save_model(\n",
    "    emotion_ov_model, \n",
    "    os.path.join(openvino_model_output_path, f\"face_emotion_resnet18.xml\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbef068e-f334-46d0-92ee-1b92b3edf328",
   "metadata": {},
   "source": [
    "gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d80d9ee-5382-48b5-b976-dfbe70f0b92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert model to openvino.runtime.Model object\n",
    "\n",
    "openvino_model_output_path = os.path.join(\n",
    "    os.path.dirname(face_api_manager._gender_detector._torch_model_path),\n",
    "    \"face_gender_openvino_model\"\n",
    ")\n",
    "\n",
    "# Save openvino.runtime.Model object on disk\n",
    "gender_ov_model = ov.convert_model(face_api_manager._gender_detector._model)\n",
    "ov.save_model(\n",
    "    gender_ov_model, \n",
    "    os.path.join(openvino_model_output_path, f\"face_gender_resnet18.xml\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4438dffe-bb2f-4fd8-80eb-1cc298923696",
   "metadata": {},
   "source": [
    "age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "346d1927-9693-4ff4-94ad-f58f8f56ac8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert model to openvino.runtime.Model object\n",
    "age_ov_model = ov.convert_model(face_api_manager._age_detector._model)\n",
    "openvino_model_output_path = os.path.join(\n",
    "    os.path.dirname(face_api_manager._age_detector._torch_model_path),\n",
    "    \"face_age_openvino_model\"\n",
    ")\n",
    "# Save openvino.runtime.Model object on disk\n",
    "ov.save_model(\n",
    "    age_ov_model, \n",
    "    os.path.join(openvino_model_output_path, f\"face_age_resnet18.xml\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadfb213-4318-4800-87fe-524546435f22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c950fb-143e-40b3-8d54-bbc6f7f6c814",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b82b722-309a-4203-bbe3-25cbfd741dc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d605a660-a943-4287-a924-dd4e3a5c4bde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
